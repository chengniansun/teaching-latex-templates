@misc{typeattr,
	author       = {GCC},
	title        = {Type Attributes},
	year         = {2022},
	url          = {https://gcc.gnu.org/onlinedocs/gcc/Type-Attributes.html},
	lastaccessed = {February 28, 2022},
	howpublished = "\url{https://gcc.gnu.org/onlinedocs/gcc/Type-Attributes.html}",
	Note         = {Online; accessed 26-March-2022}
}
@misc{functionattr,
	author       = {GCC},
	title        = {Function Attributes},
	year         = {2022},
	url          = {https://gcc.gnu.org/onlinedocs/gcc/Function-Attributes.html},
	lastaccessed = {February 28, 2022},
	howpublished = {\url{https://gcc.gnu.org/onlinedocs/gcc/Function-Attributes.html}},
	Note         = {Online; accessed 26-March-2022}
}
@misc{variableattr,
	author       = {GCC},
	title        = {Variable Attributes},
	year         = {2022},
	url          = {https://gcc.gnu.org/onlinedocs/gcc/Variable-Attributes.html},
	lastaccessed = {February 28, 2022},
	howpublished = {\url{https://gcc.gnu.org/onlinedocs/gcc/Variable-Attributes.html}},
	Note         = {Online; accessed 26-March-2022}
}
@misc{labelattr,
	author       = {GCC},
	title        = {Label Attributes},
	year         = {2022},
	url          = {https://gcc.gnu.org/onlinedocs/gcc/Label-Attributes.html},
	lastaccessed = {February 28, 2022},
	howpublished = {\url{https://gcc.gnu.org/onlinedocs/gcc/Label-Attributes.html}},
	Note         = {Online; accessed 26-March-2022}
}
@misc{linecontrol,
	author       = {GCC},
	title        = {Line Control},
	year         = {2022},
	url          = {https://gcc.gnu.org/onlinedocs/cpp/Line-Control.html},
	lastaccessed = {February 28, 2022},
	howpublished = {\url{https://gcc.gnu.org/onlinedocs/cpp/Line-Control.html}},
	Note         = {Online; accessed 26-March-2022}
}
@misc{optoption,
	author       = {GCC},
	title        = {Optimize Options},
	year         = {2022},
	url          = {https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html},
	lastaccessed = {February 28, 2022},
	howpublished = {\url{https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html}},
	Note         = {Online; accessed 26-March-2022}
}
@misc{coredump,
	author       = {Wikipedia},
	title        = {Core dump},
	year         = {2022},
	url          = {https://en.wikipedia.org/wiki/Core_dump},
	lastaccessed = {February 28, 2022},
	howpublished = {\url{https://en.wikipedia.org/wiki/Core_dump}},
	Note         = {Online; accessed 26-March-2022}
}
@misc{clangdebuglevel,
	author       = {The LLVM Compiler Infrastructure Project},
	title        = {Controlling Debug Information},
	year         = {2022},
	url          = {https://clang.llvm.org/docs/ClangCommandLineReference.html#kind-and-level-of-debug-information},
	lastaccessed = {February 28, 2022},
	howpublished = {\url{https://clang.llvm.org/docs/ClangCommandLineReference.html#kind-and-level-of-debug-information}},
	Note         = {Online; accessed 26-March-2022}
}
@misc{gccdebuglevel,
	author       = {The GDB developers},
	title        = {Options for Debugging Your Program},
	year         = {2022},
	url          = {https://gcc.gnu.org/onlinedocs/gcc/Debugging-Options.html},
	lastaccessed = {February 28, 2022},
	howpublished = {\url{https://gcc.gnu.org/onlinedocs/gcc/Debugging-Options.html}},
	Note         = {Online; accessed 26-March-2022}
}
@misc{DWARF,
	author       = {DWARF Debugging Information Format Committee},
	title        = {DWARF Debugging Information Format Version 5},
	year         = {2017},
	url          = {https://dwarfstd.org/doc/DWARF5.pdf},
	lastaccessed = {February 28, 2022},
	howpublished = "\url{https://dwarfstd.org/doc/DWARF5.pdf}",
	Note         = {Online; accessed 26-March-2022}
}
@techreport{menapace1992stabs,
	title={The” stabs” debug format},
	author={Menapace, Julia and Kingdon, Jim and MacKenzie, David},
	year={1992},
	institution={Technical report, Cygnus support}
}
@misc{PDB,
	author       = {Microsoft},
	title        = {The PDB (Program Database) Symbol File format},
	year         = {2022},
	url          = {https://github.com/Microsoft/microsoft-pdb},
	lastaccessed = {February 28, 2022},
	howpublished = {\url{https://github.com/Microsoft/microsoft-pdb}},
	Note         = {Online; accessed 26-Mar-2022}
}
@misc{GDB,
	author       = {The GDB developers},
	title        = {GDB: The GNU Project Debugger},
	year         = {2022},
	url          = {https://www.sourceware.org/gdb/},
	lastaccessed = {February 28, 2022},
	howpublished = {\url{https://www.sourceware.org/gdb/}},
	Note         = {Online; accessed 26-Mar-2022}
}
@misc{LLDB,
	author       = {The LLVM Compiler Infrastructure Project},
	title        = {The LLDB Debugger},
	year         = {2022},
	url          = {https://lldb.llvm.org/},
	lastaccessed = {February 28, 2022},
	howpublished = {\url{https://lldb.llvm.org/}},
	Note         = {Online; accessed 26-Mar-2022}
}
@misc{realllvm-ccmd-quote,
    author       = {Paul Robinson},
    title        = {Bug 37306 - [fuzzDI] -O1 + `-g' cause the generated code to change.},
    year         = {2018},
    url          = {https://bugs.llvm.org/show_bug.cgi?id=37306#c7},
    lastaccessed = {February 28, 2022},
    howpublished = {\url{https://bugs.llvm.org/show_bug.cgi?id=37306#c7}},
    Note         = {Online; accessed 26-Mar-2022}
}
@misc{llvm-ccmd-quote,
	author       = {A developer of LLVM},
	title        = {Hidden for double-blind review},
	year         = {2018},
	url          = {url-is-hidden-for-double-blind-review},
	lastaccessed = {February 28, 2022},
	howpublished = {\url{url-is-hidden-for-double-blind-review}}},
	Note         = {Online; accessed 26-Mar-2022}
}
@inproceedings{Li2020,
	author = {Li, Yuanbo and Ding, Shuo and Zhang, Qirun and Italiano, Davide},
	title = {Debug Information Validation for Optimized Code},
	year = {2020},
	isbn = {9781450376136},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3385412.3386020},
	doi = {10.1145/3385412.3386020},
	abstract = {Almost all modern production software is compiled with optimization. Debugging optimized code is a desirable functionality. For example, developers usually perform post-mortem debugging on the coredumps produced by software crashes. Designing reliable debugging techniques for optimized code has been well-studied in the past. However, little is known about the correctness of the debug information generated by optimizing compilers when debugging optimized code. Optimizing compilers emit debug information (e.g., DWARF information) to support source code debuggers. Wrong debug information causes debuggers to either crash or to display wrong variable values. Existing debugger validation techniques only focus on testing the interactive aspect of debuggers for dynamic languages (i.e., with unoptimized code). Validating debug information for optimized code raises some unique challenges: (1) many breakpoints cannot be reached by debuggers due to code optimization; and (2) inspecting some arbitrary variables such as uninitialized variables introduces undefined behaviors. This paper presents the first generic framework for systematically testing debug information with optimized code. We introduce a novel concept called actionable program. An actionable program P⟨ s, v⟩ contains a program location s and a variable v to inspect. Our key insight is that in both the unoptimized program P⟨ s,v⟩ and the optimized program P⟨ s,v⟩′, debuggers should be able to stop at the program location s and inspect the value of the variable v without any undefined behaviors. Our framework generates actionable programs and does systematic testing by comparing the debugger output of P⟨ s, v⟩′ and the actual value of v at line s in P⟨ s, v⟩. We have applied our framework to two mainstream optimizing C compilers (i.e., GCC and LLVM). Our framework has led to 47 confirmed bug reports, 11 of which have already been fixed. Moreover, in three days, our technique has found 2 confirmed bugs in the Rust compiler. The results have demonstrated the effectiveness and generality of our framework.},
	booktitle = {Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation},
	pages = {1052–1065},
	numpages = {14},
	keywords = {Debug Information, Optimizing Compilers},
	location = {London, UK},
	series = {PLDI 2020}
}
@article{hennessy82,
	author = {Hennessy, John},
	title = {Symbolic Debugging of Optimized Code},
	year = {1982},
	issue_date = {July 1982},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {4},
	number = {3},
	issn = {0164-0925},
	url = {https://doi.org/10.1145/357172.357173},
	doi = {10.1145/357172.357173},
	journal = {ACM Trans. Program. Lang. Syst.},
	month = {jul},
	pages = {323–344},
	numpages = {22}
}
@inproceedings{emi,
	author    = {Vu Le and
	Mehrdad Afshari and
	Zhendong Su},
	editor    = {Michael F. P. O'Boyle and
	Keshav Pingali},
	title     = {Compiler validation via equivalence modulo inputs},
	booktitle = {{ACM} {SIGPLAN} Conference on Programming Language Design and Implementation,
	{PLDI} '14, Edinburgh, United Kingdom - June 09 - 11, 2014},
	pages     = {216--226},
	publisher = {{ACM}},
	year      = {2014},
	url       = {https://doi.org/10.1145/2594291.2594334},
	doi       = {10.1145/2594291.2594334},
	timestamp = {Tue, 30 Nov 2021 15:21:24 +0100},
	biburl    = {https://dblp.org/rec/conf/pldi/LeAS14.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{orion,
	author    = {Vu Le and
	Chengnian Sun and
	Zhendong Su},
	editor    = {Jonathan Aldrich and
	Patrick Eugster},
	title     = {Finding deep compiler bugs via guided stochastic program mutation},
	booktitle = {Proceedings of the 2015 {ACM} {SIGPLAN} International Conference on
	Object-Oriented Programming, Systems, Languages, and Applications,
	{OOPSLA} 2015, part of {SPLASH} 2015, Pittsburgh, PA, USA, October
	25-30, 2015},
	pages     = {386--399},
	publisher = {{ACM}},
	year      = {2015},
	url       = {https://doi.org/10.1145/2814270.2814319},
	doi       = {10.1145/2814270.2814319},
	timestamp = {Tue, 30 Nov 2021 15:21:24 +0100},
	biburl    = {https://dblp.org/rec/conf/oopsla/LeSS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{csmith,
	author = {Yang, Xuejun and Chen, Yang and Eide, Eric and Regehr, John},
	title = {Finding and Understanding Bugs in C Compilers},
	year = {2011},
	isbn = {9781450306638},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/1993498.1993532},
	doi = {10.1145/1993498.1993532},
	abstract = {Compilers should be correct. To improve the quality of C compilers, we created Csmith, a randomized test-case generation tool, and spent three years using it to find compiler bugs. During this period we reported more than 325 previously unknown bugs to compiler developers. Every compiler we tested was found to crash and also to silently generate wrong code when presented with valid input. In this paper we present our compiler-testing tool and the results of our bug-hunting study. Our first contribution is to advance the state of the art in compiler testing. Unlike previous tools, Csmith generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrong-code bugs. Our second contribution is a collection of qualitative and quantitative results about the bugs we have found in open-source C compilers.},
	booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
	pages = {283–294},
	numpages = {12},
	keywords = {random testing, automated testing, compiler testing, compiler defect, random program generation},
	location = {San Jose, California, USA},
	series = {PLDI '11}
}
@article{YARPGen,
author = {Livinskii, Vsevolod and Babokin, Dmitry and Regehr, John},
title = {Random Testing for C and C++ Compilers with YARPGen},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428264},
doi = {10.1145/3428264},
abstract = {Compilers should not crash and they should not miscompile applications. Random testing is an effective method for finding compiler bugs that have escaped other kinds of testing. This paper presents Yet Another Random Program Generator (YARPGen), a random test-case generator for C and C++ that we used to find and report more than 220 bugs in GCC, LLVM, and the Intel® C++ Compiler. Our research contributions include a method for generating expressive programs that avoid undefined behavior without using dynamic checks, and generation policies, a mechanism for increasing diversity of generated code and for triggering more optimizations. Generation policies decrease the testing time to find hard-to-trigger compiler bugs and, for the kinds of scalar optimizations YARPGen was designed to stress-test, increase the number of times these optimizations are applied by the compiler by an average of 20% for LLVM and 40% for GCC. We also created tools for automating most of the common tasks related to compiler fuzzing; these tools are also useful for fuzzers other than ours.},
journal = {Proc. ACM Program. Lang.},
month = {nov},
articleno = {196},
numpages = {25},
keywords = {random program generation, random testing, compiler testing, compiler defect, automated testing}
}


@article{hermes,
	author = {Sun, Chengnian and Le, Vu and Su, Zhendong},
	title = {Finding Compiler Bugs via Live Code Mutation},
	year = {2016},
	issue_date = {October 2016},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {51},
	number = {10},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/3022671.2984038},
	doi = {10.1145/3022671.2984038},
	abstract = { Validating optimizing compilers is challenging because it is hard to generate valid test programs (i.e., those that do not expose any undefined behavior). Equivalence Modulo Inputs (EMI) is an effective, promising methodology to tackle this problem. Given a test program with some inputs, EMI mutates the program to derive variants that are semantically equivalent w.r.t. these inputs. The state-of-the-art instantiations of EMI are Orion and Athena, both of which rely on deleting code from or inserting code into code regions that are not executed under the inputs. Although both have demonstrated their ability in finding many bugs in GCC and LLVM, they are still limited due to their mutation strategies that operate only on dead code regions.  This paper presents a novel EMI technique that allows mutation in the entire program (i.e., both live and dead regions). By removing the restriction of mutating only the dead regions, our technique significantly increases the EMI variant space. It also helps to more thoroughly stress test compilers as compilers must optimize mutated live code, whereas mutated dead code might be eliminated. Finally, our technique also makes compiler bugs more noticeable as miscompilations on mutated dead code may not be observable.  We have realized the proposed technique in Hermes. The evaluation demonstrates Hermes’s effectiveness. In 13 months, Hermes found 168 confirmed, valid bugs in GCC and LLVM, of which 132 have already been fixed. },
	journal = {SIGPLAN Not.},
	month = {oct},
	pages = {849–863},
	numpages = {15},
	keywords = {miscompilation, automated testing, equivalent program variants, Compiler testing}
}

@article{classfuzz,
	author = {Chen, Yuting and Su, Ting and Sun, Chengnian and Su, Zhendong and Zhao, Jianjun},
	title = {Coverage-Directed Differential Testing of JVM Implementations},
	year = {2016},
	issue_date = {June 2016},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {51},
	number = {6},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/2980983.2908095},
	doi = {10.1145/2980983.2908095},
	abstract = { Java virtual machine (JVM) is a core technology, whose reliability is critical. Testing JVM implementations requires painstaking effort in designing test classfiles (*.class) along with their test oracles. An alternative is to employ binary fuzzing to differentially test JVMs by blindly mutating seeding classfiles and then executing the resulting mutants on different JVM binaries for revealing inconsistent behaviors. However, this blind approach is not cost effective in practice because most of the mutants are invalid and redundant. This paper tackles this challenge by introducing classfuzz, a coverage-directed fuzzing approach that focuses on representative classfiles for differential testing of JVMs’ startup processes. Our core insight is to (1) mutate seeding classfiles using a set of predefined mutation operators (mutators) and employ Markov Chain Monte Carlo (MCMC) sampling to guide mutator selection, and (2) execute the mutants on a reference JVM implementation and use coverage uniqueness as a discipline for accepting representative ones. The accepted classfiles are used as inputs to differentially test different JVM implementations and find defects. We have implemented classfuzz and conducted an extensive evaluation of it against existing fuzz testing algorithms. Our evaluation results show that classfuzz can enhance the ratio of discrepancy-triggering classfiles from 1.7% to 11.9%. We have also reported 62 JVM discrepancies, along with the test classfiles, to JVM developers. Many of our reported issues have already been confirmed as JVM defects, and some even match recent clarifications and changes to the Java SE 8 edition of the JVM specification. },
	journal = {SIGPLAN Not.},
	month = {jun},
	pages = {85–99},
	numpages = {15},
	keywords = {fuzz testing, Java virtual machine, MCMC sampling, Differential testing}
}
@inproceedings{cnicse16,
	author = {Sun, Chengnian and Le, Vu and Su, Zhendong},
	title = {Finding and Analyzing Compiler Warning Defects},
	year = {2016},
	isbn = {9781450339001},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2884781.2884879},
	doi = {10.1145/2884781.2884879},
	abstract = {Good compiler diagnostic warnings facilitate software development as they indicate likely programming mistakes or code smells. However, due to compiler bugs, the warnings may be erroneous, superfluous or missing, even for mature production compilers like GCC and Clang. In this paper, we (1) propose the first randomized differential testing technique to detect compiler warning defects and (2) describe our extensive evaluation in finding warning defects in widely-used C compilers.At the high level, our technique starts with generating random programs to trigger compilers to emit a variety of compiler warnings, aligns the warnings from different compilers, and identifies inconsistencies as potential bugs. We develop effective techniques to overcome three specific challenges: (1) How to generate random programs, (2) how to align textual warnings, and (3) how to reduce test programs for bug reporting?Our technique is very effective --- we have found and reported 60 bugs for GCC (38 confirmed, assigned or fixed) and 39 for Clang (14 confirmed or fixed). This case study not only demonstrates our technique's effectiveness, but also highlights the need to continue improving compilers' warning support, an essential, but rather neglected aspect of compilers.},
	booktitle = {Proceedings of the 38th International Conference on Software Engineering},
	pages = {203–213},
	numpages = {11},
	location = {Austin, Texas},
	series = {ICSE '16}
}

@inproceedings{creducer,
	author = {Regehr, John and Chen, Yang and Cuoq, Pascal and Eide, Eric and Ellison, Chucky and Yang, Xuejun},
	title = {Test-Case Reduction for C Compiler Bugs},
	year = {2012},
	isbn = {9781450312059},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2254064.2254104},
	doi = {10.1145/2254064.2254104},
	abstract = {To report a compiler bug, one must often find a small test case that triggers the bug. The existing approach to automated test-case reduction, delta debugging, works by removing substrings of the original input; the result is a concatenation of substrings that delta cannot remove. We have found this approach less than ideal for reducing C programs because it typically yields test cases that are too large or even invalid (relying on undefined behavior). To obtain small and valid test cases consistently, we designed and implemented three new, domain-specific test-case reducers. The best of these is based on a novel framework in which a generic fixpoint computation invokes modular transformations that perform reduction operations. This reducer produces outputs that are, on average, more than 25 times smaller than those produced by our other reducers or by the existing reducer that is most commonly used by compiler developers. We conclude that effective program reduction requires more than straightforward delta debugging.},
	booktitle = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation},
	pages = {335–346},
	numpages = {12},
	keywords = {compiler testing, bug reporting, compiler defect, automated testing, test-case minimization, random testing},
	location = {Beijing, China},
	series = {PLDI '12}
}
@inproceedings{perses,
	author = {Sun, Chengnian and Li, Yuanbo and Zhang, Qirun and Gu, Tianxiao and Su, Zhendong},
	title = {Perses: Syntax-Guided Program Reduction},
	year = {2018},
	publisher = {Association for Computing Machinery},
	doi = {10.1145/3180155.3180236},
	booktitle = {Proceedings of the 40th International Conference on Software Engineering},
	pages = {361–371},
}

@inproceedings{DBDB,
author = {Lehmann, Daniel and Pradel, Michael},
title = {Feedback-Directed Differential Testing of Interactive Debuggers},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236037},
doi = {10.1145/3236024.3236037},
abstract = {To understand, localize, and fix programming errors, developers often rely on interactive debuggers. However, as debuggers are software, they may themselves have bugs, which can make debugging unnecessarily hard or even cause developers to reason about bugs that do not actually exist in their code. This paper presents the first automated testing technique for interactive debuggers. The problem of testing debuggers is fundamentally different from the well-studied problem of testing compilers because debuggers are interactive and because they lack a specification of expected behavior. Our approach, called DBDB, generates debugger actions to exercise the debugger and records traces that summarize the debugger's behavior. By comparing traces of multiple debuggers with each other, we find diverging behavior that points to bugs and other noteworthy differences. We evaluate DBDB on the JavaScript debuggers of Firefox and Chromium, finding 19 previously unreported bugs, eight of which are already fixed by the developers.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {610–620},
numpages = {11},
keywords = {JavaScript, Interactive debuggers, Differential testing},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}
@inbook{10.1145/3293882.3330567,
	author = {Tolksdorf, Sandro and Lehmann, Daniel and Pradel, Michael},
	title = {Interactive Metamorphic Testing of Debuggers},
	year = {2019},
	isbn = {9781450362245},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3293882.3330567},
	abstract = {When improving their code, developers often turn to interactive debuggers. The correctness of these tools is crucial, because bugs in the debugger itself may mislead a developer, e.g., to believe that executed code is never reached or that a variable has another value than in the actual execution. Yet, debuggers are difficult to test because their input consists of both source code and a sequence of debugging actions, such as setting breakpoints or stepping through code. This paper presents the first metamorphic testing approach for debuggers. The key idea is to transform both the debugged code and the debugging actions in such a way that the behavior of the original and the transformed inputs should differ only in specific ways. For example, adding a breakpoint should not change the control flow of the debugged program. To support the interactive nature of debuggers, we introduce interactive metamorphic testing. It differs from traditional metamorphic testing by determining the input transformation and the expected behavioral change it causes while the program under test is running. Our evaluation applies the approach to the widely used debugger in the Chromium browser, where it finds eight previously unknown bugs with a true positive rate of 51%. All bugs have been confirmed by the developers, and one bug has even been marked as release-blocking.},
	booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
	pages = {273–283},
	numpages = {11}
}
@inproceedings{10.1109/ICSE.2019.00061,
	author = {Yang, Yibiao and Zhou, Yuming and Sun, Hao and Su, Zhendong and Zuo, Zhiqiang and Xu, Lei and Xu, Baowen},
	title = {Hunting for Bugs in Code Coverage Tools via Randomized Differential Testing},
	year = {2019},
	publisher = {IEEE Press},
	url = {https://doi.org/10.1109/ICSE.2019.00061},
	doi = {10.1109/ICSE.2019.00061},
	abstract = {Reliable code coverage tools are critically important as it is heavily used to facilitate many quality assurance activities, such as software testing, fuzzing, and debugging. However, little attention has been devoted to assessing the reliability of code coverage tools. In this study, we propose a randomized differential testing approach to hunting for bugs in the most widely used C code coverage tools. Specifically, by generating random input programs, our approach seeks for inconsistencies in code coverage reports produced by different code coverage tools, and then identifies inconsistencies as potential code coverage bugs. To effectively report code coverage bugs, we addressed three specific challenges: (1) How to filter out duplicate test programs as many of them triggering the same bugs in code coverage tools; (2) how to automatically reduce large test programs to much smaller ones that have the same properties; and (3) how to determine which code coverage tools have bugs? The extensive evaluations validate the effectiveness of our approach, resulting in 42 and 28 confirmed/fixed bugs for gcov and llvm-cov, respectively. This case study indicates that code coverage tools are not as reliable as it might have been envisaged. It not only demonstrates the effectiveness of our approach, but also highlights the need to continue improving the reliability of code coverage tools. This work opens up a new direction in code coverage validation which calls for more attention in this area.},
	booktitle = {Proceedings of the 41st International Conference on Software Engineering},
	pages = {488–499},
	numpages = {12},
	keywords = {bug detection, code coverage, coverage tools, differential testing},
	location = {Montreal, Quebec, Canada},
	series = {ICSE '19}
}
@inbook{asplos-debugging,
	author = {Di Luna, Giuseppe Antonio and Italiano, Davide and Massarelli, Luca and \"{O}sterlund, Sebastian and Giuffrida, Cristiano and Querzoni, Leonardo},
	title = {Who’s Debugging the Debuggers? Exposing Debug Information Bugs in Optimized Binaries},
	year = {2021},
	isbn = {9781450383172},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3445814.3446695},
	abstract = {Despite the advancements in software testing, bugs still plague deployed software and result in crashes in production. When debugging issues —sometimes caused by “heisenbugs”— there is the need to interpret core dumps and reproduce the issue offline on the same binary deployed. This requires the entire toolchain (compiler, linker, debugger) to correctly generate and use debug information. Little attention has been devoted to checking that such information is correctly preserved by modern toolchains’ optimization stages. This is particularly important as managing debug information in optimized production binaries is non-trivial, often leading to toolchain bugs that may hinder post-deployment debugging efforts. In this paper, we present Debug2, a framework to find debug information bugs in modern toolchains. Our framework feeds random source programs to the target toolchain and surgically compares the debugging behavior of their optimized/unoptimized binary variants. Such differential analysis allows Debug2 to check invariants at each debugging step and detect bugs from invariant violations. Our invariants are based on the (in)consistency of common debug entities, such as source lines, stack frames, and function arguments. We show that, while simple, this strategy yields powerful cross-toolchain and cross-language invariants, which can pinpoint several bugs in modern toolchains. We have used Debug2 to find 23 bugs in the LLVM toolchain (clang/lldb), 8 bugs in the GNU toolchain (GCC/gdb), and 3 in the Rust toolchain (rustc/lldb)—with 14 bugs already fixed by the developers.},
	booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
	pages = {1034–1045},
	numpages = {12}
}
@inbook{dragonBook,
	author = {Aho, Alfred V. and Lam, Monica S. and Sethi, Ravi and Ullman, Jeffrey D.},
	isbn = {0321486811},
	keywords = {compilers},
	month = {August},
	posted-at = {2009-05-19 16:04:16},
	priority = {2},
	publisher = {{Addison Wesley}},
	timestamp = {2009-05-19T18:03:27.000+0200},
	title = {Compilers: Principles, Techniques, and Tools (2nd Edition)},
	year = 2006,
	chapter = {6. Intermediate-Code Generation}
}
@article{wilcoxontest,
	ISSN = {00994987},
	author = {Frank Wilcoxon},
	journal = {Biometrics Bulletin},
	number = {6},
	pages = {80--83},
	publisher = {[International Biometric Society, Wiley]},
	title = {Individual Comparisons by Ranking Methods},
	volume = {1},
	year = {1945}
}
@inproceedings{10.1145/3503222.3507764,
	author = {Theodoridis, Theodoros and Rigger, Manuel and Su, Zhendong},
	title = {Finding Missed Optimizations through the Lens of Dead Code Elimination},
	year = {2022},
	isbn = {9781450392051},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3503222.3507764},
	doi = {10.1145/3503222.3507764},
	abstract = {Compilers are foundational software development tools and incorporate increasingly sophisticated optimizations. Due to their complexity, it is difficult to systematically identify opportunities for improving them. Indeed, the automatic discovery of missed optimizations has been an important and significant challenge. The few existing approaches either cannot accurately pinpoint missed optimizations or target only specific analyses. This paper tackles this challenge by introducing a novel, effective approach that --- in a simple and general manner --- automatically identifies a wide range of missed optimizations. Our core insight is to leverage dead code elimination (DCE) to both analyze how well compilers optimize code and identify missed optimizations: (1) insert "optimization markers" in the basic blocks of a given program, (2) compute the program's live/dead basic blocks using the "optimization markers", and (3) identify missed optimizations from how well compilers eliminate dead blocks. We essentially exploit that, since DCE heavily depends on the rest of the optimization pipeline, through the lens of DCE, one can systematically quantify how well compilers optimize code. We conduct an extensive analysis of GCC and LLVM using our approach, which (1) provides quantitative and qualitative insights regarding their optimization capabilities, and (2) uncovers a diverse set of missed optimizations. Our results also lead to 84 bug reports for GCC and LLVM, of which 62 have already been confirmed or fixed, demonstrating our work's strong practical utility. We expect that the simplicity and generality of our approach will make it widely applicable for understanding compiler performance and finding missed optimizations. This work opens and initiates this promising direction.},
	booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
	pages = {697–709},
	numpages = {13},
	keywords = {compilers, missed optimizations, testing},
	location = {Lausanne, Switzerland},
	series = {ASPLOS 2022}
}
@inproceedings{10.1145/3445814.3446751,
	author = {Kasampalis, Theodoros and Park, Daejun and Lin, Zhengyao and Adve, Vikram S. and Ro\c{s}u, Grigore},
	title = {Language-Parametric Compiler Validation with Application to LLVM},
	year = {2021},
	isbn = {9781450383172},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3445814.3446751},
	doi = {10.1145/3445814.3446751},
	abstract = {We propose a new design for a Translation Validation (TV) system geared towards practical use with modern optimizing compilers, such as LLVM. Unlike existing TV systems, which are custom-tailored for a particular sequence of transformations and a specific, common language for input and output programs, our design clearly separates the transformation-specific components from the rest of the system, and generalizes the transformation-independent components. Specifically, we present Keq, the first program equivalence checker that is parametric to the input and output language semantics and has no dependence on the transformation between the input and output programs. The Keq algorithm is based on a rigorous formalization, namely cut-bisimulation, and is proven correct. We have prototyped a TV system for the Instruction Selection pass of LLVM, being able to automatically prove equivalence for translations from LLVM IR to the MachineIR used in compiling to x86-64. This transformation uses different input and output languages, and as such has not been previously addressed by the state of the art. An experimental evaluation shows that Keq successfully proves correct the translation of over 90% of 4732 supported functions in GCC from SPEC 2006.},
	booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
	pages = {1004–1019},
	numpages = {16},
	keywords = {Program Equivalence, Compilers, Simulation, Translation Validation},
	location = {Virtual, USA},
	series = {ASPLOS 2021}
}

@inproceedings{tkfuzz,
    author = {Sun, Chengnian and Le, Vu and Zhang, Qirun and Su, Zhendong},
    title = {Toward Understanding Compiler Bugs in GCC and LLVM},
    year = {2016},
    isbn = {9781450343909},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2931037.2931074},
    doi = {10.1145/2931037.2931074},
    booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
    pages = {294–305},
    numpages = {12},
    keywords = {compiler bugs, empirical studies, compiler testing},
    location = {Saarbr\"{u}cken, Germany},
    series = {ISSTA 2016}
}
